/*
 * Threading support.
 *
 * While it would be desirable to define many functions and classes declared below inline,
 * unfortunately SIMH code does not feel well about <windows.h> or other system-dependent headers
 * included before SIMH headers, because of definition conflicts.
 *
 * This is the sole reason these facilities are defined not inline, but implmented in a separate source file,
 * why we have to split smp_lock from smp_lock_impl,
 * and why we consequentially use smp_lock::create() to create a critical section object
 * instead of defining it statically, and why invocations of all functions defined in <windows.h>
 * have to be made from a separate file and cannot be defined as a macro or inline function
 * in header files.
 */

#if !defined(__linux) && !defined(_WIN32) && !defined(__APPLE__)
#  error "sim_threads: unsupported operating system"
#endif

void init_threads_core();
void init_threads_ext();

#if defined(_WIN32) && defined(_M_IX86)
#  define __i386__  1
#endif

#if defined(_WIN64) && defined(_M_X64)
#  define __x86_64__  1
#endif

#if defined(__i386__)
#  define __x86_32__  1
#endif

#if !defined(__x86_32__) && !defined(__x86_64__)
#  error "sim_threads: unsupported target processor architecture"
#endif

/*
 * Define compiler reordering barrier
 */
#if defined(_WIN32)
#  include <intrin.h>
   // void _ReadWriteBarrier();
   /*
    * _ReadWriteBarrier() is a compiler-only barrier, 
    * it does not generate any memory barrier instructions for CPU hardware,
    * it only prevents compiler from re-ordering instructions
    * and makes compiler assume global variables might have been changed.
    */
#  define COMPILER_BARRIER _ReadWriteBarrier()
#endif

#if defined(__linux) || defined(__APPLE__)
#  define COMPILER_BARRIER __asm__ __volatile__ ("" ::: "memory")
#endif

/* compiler barrier */
#define barrier()  do { COMPILER_BARRIER; } while (0)
// void barrier();

/* 
 * SMP_NATIVE_INTERLOCKED defines whether native implementation for VAX interlocked
 * instructions is available and should be compiled into the executable. Value is boolean (1/0).
 *
 * SMP_NATIVE_INTERLOCKED set to 1 does not mean that native interlock will actually
 * be used, just that it is available as an option that can be toggled on at runtime.
 *
 * Actual use of native/portable interlock modes is controlled by flag "use_native_interlocked".
 */
#if !defined(SMP_NATIVE_INTERLOCKED) && (defined(__x86_32__) || defined(__x86_64__))
#  define SMP_NATIVE_INTERLOCKED  1
#endif

#if !defined(SMP_NATIVE_INTERLOCKED)
#  define SMP_NATIVE_INTERLOCKED  0
#endif

int32 smp_native_adawi(volatile void* memory, int32 pa_sum, uint16 addend);
t_bool smp_native_bb(volatile void* memory, int32 pa, int32 bit, t_bool set);

/*
 * Defines whether employed x86/x64 compiler can emit code that utilizies SSE/3DNow
 * non-temporal instructions, or whether non-temporal instructions are used by runtime libraries,
 * without proper internal termination with LFENCE/SFENCE/MFENCE instructon.
 * 
 * If non-temporal instructions are generated by compilers or used in runtime libraries
 * without xFENCE termination, this termination has to be supplied by VAX MP as a part of
 * memory barriers. In this case define SMP_X86_USING_NONTEMPORAL as 0.
 * 
 * Otheriwse define SMP_X86_USING_NONTEMPORAL as 0.
 * 
 * To the best of our knowledge:
 * 
 *    GLIBC uses non-termporal instructions but brackets then with fences.
 *    There was a bug in old GLIBC versions (prior to the fix in 2.5.24) when fences were not
 *    performed.
 * 
 *    GCC may emit non-termporal instructions but brackets then with fences.
 * 
 *    MSVC/CRTL 9.0 and Rtl functons invoked by CRTL in Windows XP do not use non-temporal
 *    instructions. We expect later versions either not use them or bracket then with
 *    fences, otherwise it would have been a gross bug.
 * 
 */
#if defined(__x86_32__) || defined(__x86_64__)
#  define SMP_X86_USING_NONTEMPORAL  1
#endif

/*
 * Maximum cache line size.
 *
 * For x86/x64 see "Intel 64 and IA-32 Architectures Software Developer’s Manual",
 *                 Volume 3 (3A & 3B, System Programming Guide)
 *     Table 11-1 "Characteristics of the Caches, TLBs, Store Buffer, and
 *                 Write Combining Buffer in Intel 64 and IA-32 Processors"
 *     and section 8.10.6.7 "Place Locks and Semaphores in Aligned, 128-Byte Blocks of Memory"
 */
#if defined(__x86_32__) || defined(__x86_64__)
#  define SMP_MAXCACHELINESIZE 128
#endif

/* alignment and padding for cache-line aligned data */
#if defined(_WIN32)
# define SIM_ALIGN_CACHELINE __declspec(align(SMP_MAXCACHELINESIZE))
#elif defined(__GNUC__)
# define SIM_ALIGN_CACHELINE __attribute__ ((__aligned__ (SMP_MAXCACHELINESIZE)))
#else
# error Unsupported compiler
#endif

/* memory barriers */

#define SMP_RMB (1 << 0)
#define SMP_WMB (1 << 1)
#define SMP_MB (SMP_RMB | SMP_WMB)

#if defined(__x86_32__) || defined(__x86_64__)
  extern void (*smp_rmb_p)();
  extern void (*smp_wmb_p)();
  extern void (*smp_mb_p)();
#  define smp_rmb() do { barrier();  (*smp_rmb_p)();  barrier(); } while (0)
#  define smp_wmb() do { barrier();  (*smp_wmb_p)();  barrier(); } while (0)
#  define smp_mb()  do { barrier();  (*smp_mb_p)();   barrier(); } while (0)
#else
  void smp_rmb();
  void smp_wmb();
  void smp_mb();
#endif

/*
 * Read (atomic) memory location without memory barriers
 * or waiting for cache coherency protocol to complete.
 *
 * Should be applied to atomic types only, properly aligned in memory.
 *
 */
#define weak_read(m) (m)
#define weak_read_var(m) ((m).var)

/*
 * macros that define whether should execute memory barrier after host interlocked instructions,
 * or these instructions do invoke the barrier by themselves
 */
#if (defined(__x86_32__) || defined(__x86_64__)) && (SMP_X86_USING_NONTEMPORAL  == 0)
#  define smp_pre_interlocked_wmb()   barrier()
#  define smp_post_interlocked_rmb()  barrier()
#  define smp_post_interlocked_wmb()  barrier()
#  define smp_pre_interlocked_mb()    barrier()
#  define smp_post_interlocked_mb()   barrier()
#else
#  define smp_pre_interlocked_wmb()   smp_wmb()
#  define smp_post_interlocked_rmb()  smp_rmb()
#  define smp_post_interlocked_wmb()  smp_wmb()
#  define smp_pre_interlocked_mb()    smp_mb()
#  define smp_post_interlocked_mb()   smp_mb()
#endif

/* thread meta-priorities */
/* must be ordered in the order of increasing relative rank */
typedef enum
{
    SIMH_THREAD_PRIORITY_INVALID = -1,                      /* priority not set or must be recalculated */
    SIMH_THREAD_PRIORITY_CPU_RUN = 0,                       /* normal priority for VCPU thread execution */
    SIMH_THREAD_PRIORITY_IOP = 1,                           /* IO processing thread (virtual disk, tape drive or network adapter) */
    SIMH_THREAD_PRIORITY_CONSOLE_PAUSED = 2,                /* console thread while VCPUs are paused */
    SIMH_THREAD_PRIORITY_CONSOLE_RUN = 3,                   /* console thread while VCPUs are running */
    SIMH_THREAD_PRIORITY_CPU_CRITICAL_OS = 4,               /* VCPU thread inside OS critical section */
    SIMH_THREAD_PRIORITY_CPU_CRITICAL_OS_HI = 5,            /* VCPU thread inside clock interrupt or IPI interrupt processing,
                                                               or any of these interrupts is pending */
    SIMH_THREAD_PRIORITY_CPU_CRITICAL_VM = 6,               /* VCPU thread inside VM critical section */
    SIMH_THREAD_PRIORITY_CLOCK = 7,                         /* clock strobing thread */
    SIMH_THREAD_PRIORITY_CPU_CALIBRATION = 8                /* VCPU thread running calibration loop (using SSC clocks) */
}
sim_thread_priority_t;

/* thread types */
typedef enum
{
    SIM_THREAD_TYPE_CONSOLE = 0,       /* console thread */
    SIM_THREAD_TYPE_CPU = 1,           /* VCPU thread */
    SIM_THREAD_TYPE_CLOCK = 2,         /* HW clock strobing thread */
    SIM_THREAD_TYPE_IOP = 3            /* IO processor thread */
}
sim_thread_type_t;

typedef enum
{
    SIM_LOCK_CRITICALITY_NONE = 0,
    SIM_LOCK_CRITICALITY_OS_HI = 1,
    SIM_LOCK_CRITICALITY_VM = 2
}
sim_lock_criticality_t;

class sim_delta_timer;

/*
 * atomic access to variables: concurrent access retrives either old value or new value,
 * but never a partial update (such as some bytes of previous value and other bytes of new value)
 */

typedef SIM_ALIGN_32 volatile int32 atomic_int32;
typedef SIM_ALIGN_32 volatile uint32 atomic_uint32;
typedef SIM_ALIGN_32 volatile uint32 atomic_bool;

/*
 * Host system interlocked increment and CAS operations.
 */

/* Basic data types for interlocked operations */
typedef SIM_ALIGN_16 volatile uint16 smp_interlocked_uint16;
typedef SIM_ALIGN_32 volatile uint32 smp_interlocked_uint32;
typedef SIM_ALIGN_32 volatile int32 smp_interlocked_int32;
#if defined (__x86_64__)
typedef SIM_ALIGN_64 volatile t_uint64 smp_interlocked_uint64;
#endif

/*
 * Container data types for interlocked and atomic variables.
 *
 * Isolate each independently accessed interlocked and atomic variable in memory block aligned on cache line size
 * and having a size of cache line, so when these variables are accessed by processors, they do
 * not drag along data in neighbor memory locations and do not induce ping-pong cache coherency traffic
 * on inter-processor interconnect (false sharing).
 *
 * Besides hurting performance on all processors, on some processors that implement LL/SC interlocked semantics
 * interference of separate variables within cache line may cause LL/SC instructions being improperly aborted
 * and having to retry them, possibly multiple times before they can succeed, making interlocked variable
 * segregation even more critical.
 *
 * Furthermore, on Intel x86/x64 isolating interlocked variable secures it from sharing the cache line with
 * a variable that may be accessed using non-temporal instructions, which would disrupt inter-processor cache
 * synchronization.
 */
typedef SIM_ALIGN_CACHELINE volatile struct __tag_atomic_uint32_var
{
    atomic_uint32 var;
    t_byte pad[SMP_MAXCACHELINESIZE - sizeof(atomic_uint32)];
}
atomic_uint32_var;

typedef SIM_ALIGN_CACHELINE volatile struct __tag_atomic_int32_var
{
    atomic_int32 var;
    t_byte pad[SMP_MAXCACHELINESIZE - sizeof(atomic_int32)];
}
atomic_int32_var;

typedef SIM_ALIGN_CACHELINE volatile struct __tag_smp_interlocked_uint32_var
{
    smp_interlocked_uint32 var;
    t_byte pad[SMP_MAXCACHELINESIZE - sizeof(smp_interlocked_uint32)];
}
smp_interlocked_uint32_var;

typedef SIM_ALIGN_CACHELINE volatile struct __tag_smp_interlocked_int32_var
{
    smp_interlocked_int32 var;
    t_byte pad[SMP_MAXCACHELINESIZE - sizeof(smp_interlocked_int32)];
}
smp_interlocked_int32_var;

typedef SIM_ALIGN_CACHELINE volatile struct __tag_smp_interlocked_uint16_var
{
    smp_interlocked_uint16 var;
    t_byte pad[SMP_MAXCACHELINESIZE - sizeof(smp_interlocked_uint16)];
}
smp_interlocked_uint16_var;

#if defined (__x86_64__)
typedef SIM_ALIGN_CACHELINE volatile struct __tag_smp_interlocked_uint64_var
{
    smp_interlocked_uint64 var;
    t_byte pad[SMP_MAXCACHELINESIZE - sizeof(smp_interlocked_uint64)];
}
smp_interlocked_uint64_var;
#endif

#if __SIZEOF_POINTER__ == 8
typedef smp_interlocked_uint64       smp_interlocked_addr_val;
typedef smp_interlocked_uint64_var   smp_interlocked_addr_val_var;
#else
typedef smp_interlocked_uint32       smp_interlocked_addr_val;
typedef smp_interlocked_uint32_var   smp_interlocked_addr_val_var;
#endif


#define atomic_var(x)    ((x).var)
#define atomic_var_p(px) (& ((px)->var))
#define smp_var_init(v)  { (v) }

#define smp_var(x)       ((x).var)
#define smp_var_p(px)    (& ((px)->var))
#define atomic_var_init(v)  { (v) }

/*============================================= GNUC / Linux versions =============================================*/

#if defined(__GNUC__)

SIM_INLINE static uint32 smp_interlocked_increment(smp_interlocked_uint32* p)
{
    COMPILER_BARRIER;
    uint32 r = __sync_add_and_fetch(p, 1);
    COMPILER_BARRIER;
    return r;
}

SIM_INLINE static uint32 smp_interlocked_decrement(smp_interlocked_uint32* p)
{
    COMPILER_BARRIER;
    uint32 r = __sync_add_and_fetch(p, -1);
    COMPILER_BARRIER;
    return r;
}

#if defined (__x86_64__)
SIM_INLINE static t_uint64 smp_interlocked_increment(smp_interlocked_uint64* p)
{
    COMPILER_BARRIER;
    t_uint64 r = __sync_add_and_fetch(p, 1);
    COMPILER_BARRIER;
    return r;
}

SIM_INLINE static t_uint64 smp_interlocked_decrement(smp_interlocked_uint64* p)
{
    COMPILER_BARRIER;
    t_uint64 r = __sync_add_and_fetch(p, -1);
    COMPILER_BARRIER;
    return r;
}

SIM_INLINE static t_uint64 smp_interlocked_cas(smp_interlocked_uint64* p, t_uint64 old_value, t_uint64 new_value)
{
    COMPILER_BARRIER;
    t_uint64 res = __sync_val_compare_and_swap(p, old_value, new_value);
    COMPILER_BARRIER;
    return res;
}

SIM_INLINE static t_bool smp_interlocked_cas_done(smp_interlocked_uint64* p, t_uint64 old_value, t_uint64 new_value)
{
    COMPILER_BARRIER;
    t_bool res = __sync_bool_compare_and_swap(p, old_value, new_value);
    COMPILER_BARRIER;
    return res;
}
#endif

SIM_INLINE static int32 smp_interlocked_cas(smp_interlocked_int32* p, int32 old_value, int32 new_value)
{
    COMPILER_BARRIER;
    int32 res = __sync_val_compare_and_swap(p, old_value, new_value);
    COMPILER_BARRIER;
    return res;
}

SIM_INLINE static uint32 smp_interlocked_cas(smp_interlocked_uint32* p, uint32 old_value, uint32 new_value)
{
    COMPILER_BARRIER;
    uint32 res = __sync_val_compare_and_swap(p, old_value, new_value);
    COMPILER_BARRIER;
    return res;
}

SIM_INLINE static uint16 smp_interlocked_cas(smp_interlocked_uint16* p, uint16 old_value, uint16 new_value)
{
    COMPILER_BARRIER;
    uint16 res = __sync_val_compare_and_swap(p, old_value, new_value);
    COMPILER_BARRIER;
    return res;
}

SIM_INLINE static t_byte smp_interlocked_cas(volatile t_byte* p, t_byte old_value, t_byte new_value)
{
    COMPILER_BARRIER;
    t_byte res = __sync_val_compare_and_swap(p, old_value, new_value);
    COMPILER_BARRIER;
    return res;
}

SIM_INLINE static t_bool smp_interlocked_cas_done(smp_interlocked_uint32* p, uint32 old_value, uint32 new_value)
{
    COMPILER_BARRIER;
    t_bool res = __sync_bool_compare_and_swap(p, old_value, new_value);
    COMPILER_BARRIER;
    return res;
}

SIM_INLINE static t_bool smp_interlocked_cas_done(smp_interlocked_uint16* p, uint16 old_value, uint16 new_value)
{
    COMPILER_BARRIER;
    t_bool res = __sync_bool_compare_and_swap(p, old_value, new_value);
    COMPILER_BARRIER;
    return res;
}

SIM_INLINE static t_bool smp_interlocked_cas_done(volatile t_byte* p, t_byte old_value, t_byte new_value)
{
    COMPILER_BARRIER;
    t_bool res = __sync_bool_compare_and_swap(p, old_value, new_value);
    COMPILER_BARRIER;
    return res;
}

#if defined (__x86_32__) || defined (__x86_64__)
SIM_INLINE static t_bool smp_test_set_bit(smp_interlocked_uint32* p, uint32 bit)
{
    t_byte result = FALSE;

    /* should output to "=g" (result), but GCC -O2 LTO fails to compile with error message
       about unavailable register %sil/%dil */
    __asm__ __volatile__ (
        "lock; bts %1, (%2)\n\t"
        "setc %0\n\t"
        : "=a" (result)
        : "q" (bit), "q" (p)
        : "memory", "cc"
    );

    return result;
}

SIM_INLINE static t_bool smp_test_set_bit(smp_interlocked_uint16* p, uint32 bit)
{
    t_byte result = FALSE;

    __asm__ __volatile__ (
        "lock; bts %x1, (%2)\n\t"
        "setc %0\n\t"
        : "=g" (result)
        : "Q" (bit), "r" (p)
        : "memory", "cc"
    );

    return result;
}

SIM_INLINE static t_bool smp_test_clear_bit(smp_interlocked_uint32* p, uint32 bit)
{
    t_byte result = FALSE;

    /* should output to "=g" (result), but GCC -O2 LTO fails to compile with error message
       about unavailable register %sil/%dil */
    __asm__ __volatile__ (
        "lock; btr %1, (%2)\n\t"
        "setc %0\n\t"
        : "=a" (result)
        : "q" (bit), "q" (p)
        : "memory", "cc"
    );

    return result;
}

SIM_INLINE static t_bool smp_test_clear_bit(smp_interlocked_uint16* p, uint32 bit)
{
    t_byte result = FALSE;

    __asm__ __volatile__ (
        "lock; btr %x1, (%2)\n\t"
        "setc %0\n\t"
        : "=g" (result)
        : "Q" (bit), "r" (p)
        : "memory", "cc"
    );

    return result;
}

/*
 * smp_cpu_relax:
 *
 *   Insert smp_cpu_relax into spin-wait loops to prevent generation of multiple (and useless) simultaneous out-of-order
 *   read requests. Effective on Pentium 4 and higher processors, NOP on earlier Intel processors and clones.
 *   See Intel application note AP-949 "Using Spin-Loops on Intel Pentium 4 Processor and Intel Xeon Processor",
 *   P/N 248674-002 (http://software.intel.com/file/25602).
 *   May also help when spin-waitng on a Hyper-Threaded processor by releasing shared resources to a thread
 *   doing useful work, perhaps a lock holder.
 */
#define smp_cpu_relax()  do { __asm__ __volatile__ ("pause"); } while (0)
// #define smp_cpu_relax()  do { __asm__ __volatile__ ("rep; nop"); } while (0)

#endif

/*============================================= Win32 x86 and x64 versions =============================================*/

#elif defined (_WIN32)

#include <intrin.h>
#pragma intrinsic (_InterlockedExchange, _InterlockedCompareExchange, _InterlockedCompareExchange16)
#pragma intrinsic (_InterlockedIncrement, _InterlockedDecrement)

SIM_INLINE static int32 smp_interlocked_increment(smp_interlocked_int32* p)
{
    COMPILER_BARRIER;
    int32 res = _InterlockedIncrement((long*) p);
    COMPILER_BARRIER;
    return res;
}

SIM_INLINE static uint32 smp_interlocked_increment(smp_interlocked_uint32* p)
{
    COMPILER_BARRIER;
    uint32 res = _InterlockedIncrement((long*) p);
    COMPILER_BARRIER;
    return res;
}

SIM_INLINE static int32 smp_interlocked_decrement(smp_interlocked_int32* p)
{
    COMPILER_BARRIER;
    int32 res = _InterlockedDecrement((long*) p);
    COMPILER_BARRIER;
    return res;
}

SIM_INLINE static uint32 smp_interlocked_decrement(smp_interlocked_uint32* p)
{
    COMPILER_BARRIER;
    uint32 res = _InterlockedDecrement((long*) p);
    COMPILER_BARRIER;
    return res;
}

#if defined (__x86_64__)
#pragma intrinsic (_InterlockedIncrement64, _InterlockedDecrement64, _InterlockedCompareExchange64)
SIM_INLINE static t_uint64 smp_interlocked_increment(smp_interlocked_uint64* p)
{
    COMPILER_BARRIER;
    t_uint64 res = _InterlockedIncrement64((__int64*) p);
    COMPILER_BARRIER;
    return res;
}

SIM_INLINE static t_uint64 smp_interlocked_decrement(smp_interlocked_uint64* p)
{
    COMPILER_BARRIER;
    t_uint64 res = _InterlockedDecrement64((__int64*) p);
    COMPILER_BARRIER;
    return res;
}

SIM_INLINE static t_uint64 smp_interlocked_cas(smp_interlocked_uint64* p, t_uint64 old_value, t_uint64 new_value)
{
    COMPILER_BARRIER;
    t_uint64 res = (t_uint64) _InterlockedCompareExchange64((__int64 volatile*) p, (__int64) new_value, (__int64) old_value);
    COMPILER_BARRIER;
    return res;
}

SIM_INLINE static t_bool smp_interlocked_cas_done(smp_interlocked_uint64* p, t_uint64 old_value, t_uint64 new_value)
{
    return old_value == smp_interlocked_cas(p, old_value, new_value);
}
#endif

// SIM_INLINE static uint32 smp_interlocked_xchg(smp_interlocked_uint32* p, uint32 new_value)
// {
//     COMPILER_BARRIER;
//     uint32 res = _InterlockedExchange((long*) p, (long) new_value);
//     COMPILER_BARRIER;
//     return res;
// }

SIM_INLINE static uint32 smp_interlocked_cas(smp_interlocked_int32* p, int32 old_value, int32 new_value)
{
    COMPILER_BARRIER;
    uint32 res = _InterlockedCompareExchange((long volatile*) p, (long) new_value, (long) old_value);
    COMPILER_BARRIER;
    return res;
}

SIM_INLINE static uint32 smp_interlocked_cas(smp_interlocked_uint32* p, uint32 old_value, uint32 new_value)
{
    COMPILER_BARRIER;
    uint32 res = _InterlockedCompareExchange((long volatile*) p, (long) new_value, (long) old_value);
    COMPILER_BARRIER;
    return res;
}

SIM_INLINE static uint16 smp_interlocked_cas(smp_interlocked_uint16* p, uint16 old_value, uint16 new_value)
{
    COMPILER_BARRIER;
    uint16 res = _InterlockedCompareExchange16((short volatile*) p, (short) new_value, (short) old_value);
    COMPILER_BARRIER;
    return res;
}

#if defined(__x86_32__)
SIM_INLINE static t_byte smp_interlocked_cas(volatile t_byte* p, t_byte old_value, t_byte new_value)
{
   t_byte result;

    COMPILER_BARRIER;
    __asm
    {
        mov    al, old_value
        mov    edx, p
        mov    cl, new_value
        lock cmpxchg    byte ptr [edx], cl
        mov    result, al
    }
    COMPILER_BARRIER;

    return result;
}
#else
t_byte smp_interlocked_cas(volatile t_byte* p, t_byte old_value, t_byte new_value);
#endif

SIM_INLINE static t_bool smp_interlocked_cas_done(smp_interlocked_uint32* p, uint32 old_value, uint32 new_value)
{
    return old_value == smp_interlocked_cas(p, old_value, new_value);
}

SIM_INLINE static t_bool smp_interlocked_cas_done(smp_interlocked_uint16* p, uint16 old_value, uint16 new_value)
{
    return old_value == smp_interlocked_cas(p, old_value, new_value);
}

SIM_INLINE static t_bool smp_interlocked_cas_done(volatile t_byte* p, t_byte old_value, t_byte new_value)
{
    return old_value == smp_interlocked_cas(p, old_value, new_value);
}

#if defined(__x86_32__)
SIM_INLINE static t_bool smp_test_set_bit(smp_interlocked_uint16* p, uint32 bit)
{
   t_byte result = FALSE;
    COMPILER_BARRIER;
    _asm
    {
        mov    ecx, p
        mov    ebx, bit
        lock bts    word ptr [ecx], bx
        setc   result
    }
    COMPILER_BARRIER;
    return result;
}

SIM_INLINE static t_bool smp_test_set_bit(smp_interlocked_uint32* p, uint32 bit)
{
   t_byte result = FALSE;
    COMPILER_BARRIER;
    _asm
    {
        mov    ecx, p
        mov    ebx, bit
        lock bts    dword ptr [ecx], ebx
        setc   result
    }
    COMPILER_BARRIER;
    return result;
}

SIM_INLINE static t_bool smp_test_clear_bit(smp_interlocked_uint16* p, uint32 bit)
{
   t_byte result = FALSE;
    COMPILER_BARRIER;
    _asm
    {
        mov    ecx, p
        mov    ebx, bit
        lock btr    word ptr [ecx], bx
        setc   result
    }
    COMPILER_BARRIER;
    return result;
}

SIM_INLINE static t_bool smp_test_clear_bit(smp_interlocked_uint32* p, uint32 bit)
{
   t_byte result = FALSE;
    COMPILER_BARRIER;
    _asm
    {
        mov    ecx, p
        mov    ebx, bit
        lock btr    dword ptr [ecx], ebx
        setc   result
    }
    COMPILER_BARRIER;
    return result;
}
#else
#pragma intrinsic (_interlockedbittestandset, _interlockedbittestandreset)
SIM_INLINE static t_bool smp_test_set_bit(smp_interlocked_uint32* p, uint32 bit)
{
    COMPILER_BARRIER;
    t_bool r = _interlockedbittestandset((long*) p, (long) bit);
    COMPILER_BARRIER;
    return r;
}

SIM_INLINE static t_bool smp_test_clear_bit(smp_interlocked_uint32* p, uint32 bit)
{
    COMPILER_BARRIER;
    t_bool r = _interlockedbittestandreset((long*) p, (long) bit);
    COMPILER_BARRIER;
    return r;
}

SIM_INLINE static t_bool smp_test_set_bit(smp_interlocked_uint16* p, uint32 bit);
SIM_INLINE static t_bool smp_test_clear_bit(smp_interlocked_uint16* p, uint32 bit);
#endif

#if defined (__x86_32__) || defined (__x86_64__)
#  pragma intrinsic (_mm_pause)
   extern "C" void _mm_pause(void);
#  define smp_cpu_relax()  _mm_pause()
#endif

#else

/*====================================== for other platforms (not GNUC, not WIN32) ======================================*/

uint32 smp_interlocked_increment(smp_interlocked_uint32* p);
uint32 smp_interlocked_decrement(smp_interlocked_uint32* p);

#if defined (__x86_64__)
t_uint64 smp_interlocked_increment(smp_interlocked_uint64* p);
t_uint64 smp_interlocked_decrement(smp_interlocked_uint64* p);
#endif

#if !defined(smp_cpu_relax)
#  define smp_cpu_relax()
#endif

#endif

/*=================================== end of machine-specific interlock op definitions ===================================*/

/* interlocked operations for variables packed in cacheline container */
#define smp_interlocked_increment_var(p)  smp_interlocked_increment(smp_var_p(p))
#define smp_interlocked_decrement_var(p)  smp_interlocked_decrement(smp_var_p(p))
#define smp_interlocked_cas_var(p, old_value, new_value)  smp_interlocked_cas(smp_var_p(p), (old_value), (new_value))
#define smp_interlocked_cas_done_var(p, old_value, new_value)  smp_interlocked_cas_done(smp_var_p(p), (old_value), (new_value))

/* number of available online physical processors on the host system */
extern int smp_ncpus;

/* number of SMT/HyperThreading units per processor core on the host system */
extern int smp_nsmt_per_core;
extern t_bool smp_smt_factor_set;
extern double smp_smt_factor;

/* based on expected cost of rescheduling and context switching, it is worthwhile to spin-wait on a lock 
   without yielding CPU for at least this number of microseconds */
extern uint32 smp_spinwait_min_us;

/* Helpers for locking/unlocking VM-critical objects */
void critical_lock(sim_lock_criticality_t criticality);
void critical_unlock(sim_lock_criticality_t criticality);
#define vm_critical_lock()     critical_lock(SIM_LOCK_CRITICALITY_VM)
#define vm_critical_unlock()   critical_unlock(SIM_LOCK_CRITICALITY_VM)
#define os_hi_critical_lock()     critical_lock(SIM_LOCK_CRITICALITY_OS_HI)
#define os_hi_critical_unlock()   critical_unlock(SIM_LOCK_CRITICALITY_OS_HI)

class smp_lock
{
public:
    virtual t_bool init(uint32 cycles = 0, t_bool dothrow = TRUE) = 0;
    virtual t_bool init(uint32 us, uint32 min_cycles, uint32 max_cycles, t_bool dothrow = TRUE) = 0;
    virtual void set_spin_count(uint32 cycles) = 0;
    virtual void set_criticality(sim_lock_criticality_t criticality) = 0;
    virtual void lock() = 0;
    virtual void unlock() = 0;
    virtual ~smp_lock() {};
    static smp_lock* create(uint32 cycles = 0, t_bool dothrow = TRUE);
    static smp_lock* create(uint32 us, uint32 min_cycles, uint32 max_cycles, t_bool dothrow = TRUE);

    /* real-time calibration */
    static void calibrate();

    /* performance counters */
    virtual void set_perf_collect(t_bool collect) {}
    virtual void perf_reset() {}
    virtual void perf_show(SMP_FILE* fp, const char* name) {}
};

class SIM_ALIGN_CACHELINE InterruptRegister
{
protected:
    /* dynamic part written to by other threads */
    smp_interlocked_uint32* irqs;           /* irq bits set by devices and processors */
    smp_interlocked_uint32_var changed;     /* marker: irqs may have changed */

    /* dynamic part accessed locally, these variables should be updated if "changed" is set */
    uint32* local_irqs;                     /* local recent copy of "irqs" */

    /* static (after init) part */
    uint32  lo_ipl;                         /* lowest IPL in irqs array */
    uint32  hi_ipl;                         /* highest IPL in irqs array */
    const uint32* devs_per_ipl;             /* device count per each level */

public:
    InterruptRegister();
    ~InterruptRegister();
    static void* operator new(size_t size)    { return operator_new_aligned(size, SMP_MAXCACHELINESIZE); }
    static void  operator delete(void* p)     { operator_delete_aligned(p); }
    void init(uint32 lo_ipl, uint32 hi_ipl, const uint32* devs_per_ipl);
    void reset();
    t_bool weak_changed()
        { return weak_read_var(changed) != 0; }
    SIM_INLINE t_bool cas_changed(t_bool old_value, t_bool new_value)
        { return smp_interlocked_cas_done_var(& changed, (uint32) old_value, (uint32) new_value) ? old_value : !old_value; }
    void copy_irqs_to_local();
    int32 highest_local_irql();
    t_bool is_local_int(uint32 ipl, uint32 dev);
    void dismiss_int(RUN_DECL, uint32 ipl, uint32 dev);
    void query_local_clk_ipi(t_bool* is_active_clk_interrupt, t_bool* is_active_ipi_interrupt);
    t_bool set_int(uint32 ipl, uint32 dev, t_bool toself);
    t_bool clear_int(uint32 ipl, uint32 dev, t_bool toself);
    t_bool check_int_atipl_clr(RUN_DECL, uint32 ipl, uint32* int_dev);
    t_bool query_syncw_sys();
    t_bool examine_int(uint32 ipl, uint32 dev);
    // t_bool examine_int(uint32 ipl);
    // t_bool check_int(uint32 ipl, uint32* int_ipl);
    // t_bool check_int(uint32 ipl, uint32* int_ipl, uint32* int_dev);
    void show_external(SMP_FILE* st, t_bool& none);
    void show_local(SMP_FILE* st, t_bool& none);

protected:
    void show_aux(SMP_FILE* st, uint32* rqs, t_bool& none);
    void show_aux_2(SMP_FILE* st, const char* intr, t_bool& none);
};

#if defined(_WIN32)
typedef uint32 /*DWORD*/ smp_tls_key;
t_bool smp_tls_alloc(smp_tls_key* key);
void tls_set_value(const smp_tls_key& key, void* value);
void* tls_get_value(const smp_tls_key& key);
#elif defined(__linux) || defined(__APPLE__)
#  include <pthread.h>
typedef pthread_key_t smp_tls_key;
SIM_INLINE static t_bool smp_tls_alloc(smp_tls_key* key)
{
    return 0 == pthread_key_create(key, NULL);
}
SIM_INLINE static void tls_set_value(smp_tls_key& key, void* value)
{
    pthread_setspecific(key, value);
}
SIM_INLINE static void* tls_get_value(smp_tls_key& key)
{
    return pthread_getspecific(key);
}
#endif

t_bool check_aligned(void* p, uint32 alignment, t_bool dothrow = TRUE);
t_bool smp_check_aligned(const smp_interlocked_int32* p, t_bool dothrow = TRUE);
t_bool smp_check_aligned(const smp_interlocked_uint32* p, t_bool dothrow = TRUE);
t_bool smp_check_aligned(const smp_interlocked_int32_var* p, t_bool dothrow = TRUE);
t_bool smp_check_aligned(const smp_interlocked_uint32_var* p, t_bool dothrow = TRUE);
t_bool smp_check_aligned(const atomic_int32_var* p, t_bool dothrow = TRUE);
t_bool smp_check_aligned(const atomic_uint32_var* p, t_bool dothrow = TRUE);

/* from the compiler's viewpoint, atomic_int32 is the same as  smp_interlocked_int32 */
// t_bool smp_check_aligned(const atomic_int32* p, t_bool dothrow = TRUE);

#if defined (__x86_64__)
t_bool smp_check_aligned(const smp_interlocked_uint64* p, t_bool dothrow = TRUE);
t_bool smp_check_aligned(const smp_interlocked_uint64_var* p, t_bool dothrow = TRUE);
#endif

#if defined(_WIN32)
typedef void* smp_pollable_handle_t;  // HANDLE
#elif defined(__linux) || defined(__APPLE__)
typedef int smp_pollable_handle_t;
#endif

#if defined(_WIN32)
   typedef void* smp_thread_t;
   typedef unsigned int (__stdcall *smp_thread_routine_t)(void*);
#  define SMP_THREAD_NULL NULL
#  define SMP_THREAD_ROUTINE_DECL unsigned int __stdcall
#  define SMP_THREAD_ROUTINE_END return 0
#elif defined(__linux) || defined(__APPLE__)
   typedef pthread_t smp_thread_t;
   typedef void* (*smp_thread_routine_t)(void*);
#  define SMP_THREAD_NULL ((pthread_t) 0)
#  define SMP_THREAD_ROUTINE_DECL void *
#  define SMP_THREAD_ROUTINE_END do { pthread_exit(NULL); return NULL; } while (0)
#endif

typedef enum
{
    SMP_AFFINITY_ALL = 0,
    SMP_AFFINITY_PER_CORE = 1
}
smp_affinity_kind_t;

t_bool smp_thread_init();
t_bool smp_create_thread(smp_thread_routine_t start_routine, void* arg, smp_thread_t* thread_handle, t_bool dothrow = TRUE);
t_bool smp_wait_thread(smp_thread_t thread_handle, t_value* exitcode = NULL, t_bool dothrow = TRUE);
t_bool smp_set_thread_priority(sim_thread_priority_t prio);
t_bool smp_set_thread_priority(smp_thread_t thread_th, sim_thread_priority_t prio);
void smp_set_thread_name(const char* name);
int smp_get_thread_os_priority(smp_thread_t thread_th);
t_bool smp_can_alloc_per_core(int nthreads);
void smp_set_affinity(smp_thread_t thread_th, smp_affinity_kind_t how);

class smp_synch_object
{
public:
    smp_synch_object() {}
    virtual ~smp_synch_object() {}
    virtual void clear() = 0;
    virtual void wait() = 0;
    virtual t_bool trywait() = 0;
    virtual void release(int count = 1) = 0;
};

class smp_pollable_synch_object : public smp_synch_object
{
public:
    smp_pollable_synch_object() {}
    virtual ~smp_pollable_synch_object() {}
    virtual smp_pollable_handle_t pollable_handle() = 0;
    virtual const char* pollable_handle_op() = 0;
};

class smp_semaphore : public smp_pollable_synch_object
{
public:
    smp_semaphore() {}
    virtual ~smp_semaphore() {}
    static smp_semaphore* create(int initial_open_count, t_bool dothrow = TRUE);
    virtual t_bool init(int initial_open_count, t_bool dothrow = TRUE) = 0;
};

#if defined(_WIN32)
typedef smp_semaphore smp_simple_semaphore;
#else
class smp_simple_semaphore : public smp_synch_object
{
public:
    smp_simple_semaphore() {}
    virtual ~smp_simple_semaphore() {}
    static smp_simple_semaphore* create(int initial_open_count, t_bool dothrow = TRUE);
    virtual t_bool init(int initial_open_count, t_bool dothrow = TRUE) = 0;
};
#endif

class smp_barrier : public smp_synch_object
{
public:
    smp_barrier() {}
    virtual ~smp_barrier() {}
    static smp_barrier* create(int initial_count, t_bool dothrow = TRUE);
    virtual void wait() = 0;
    virtual t_bool init(int initial_count, t_bool dothrow = TRUE) = 0;
    virtual t_bool set_count(int count, t_bool dothrow = TRUE) = 0;
};

class smp_pollable_console_keyboard : public smp_pollable_synch_object
{
private:
    static smp_pollable_console_keyboard* instance;
public:
    smp_pollable_console_keyboard() {}
    virtual ~smp_pollable_console_keyboard() {}
    virtual smp_pollable_handle_t pollable_handle() = 0;
    virtual const char* pollable_handle_op() = 0;
    static smp_pollable_console_keyboard* get();
};

int smp_wait_any(smp_pollable_synch_object** objects, int nobjects, int ms);
int smp_wait(smp_pollable_synch_object* object, int ms);

#if defined(__linux) || defined(__APPLE__)
#  define DECL_RESTARTABLE(rc)  int rc;
#  define DO_RESTARTABLE(rc, op)  do {(rc) = (op);} while ((rc) == -1 && errno == EINTR)
#  define DO_RVAL_RESTARTABLE(rc, op)  do {(rc) = (op);} while ((rc) == EINTR)
#endif

class smp_mutex
{
public:
    static smp_mutex* create(t_bool dothrow = TRUE);
    virtual ~smp_mutex() {}
    virtual void set_criticality(sim_lock_criticality_t criticality) = 0;
    virtual void lock() = 0;
    virtual void unlock() = 0;
    virtual t_bool trylock() = 0;
};

class smp_condvar
{
public:
    static smp_condvar* create(t_bool dothrow = TRUE);
    virtual ~smp_condvar() {}
    virtual void prepareForWait() = 0;
    virtual void wait(smp_mutex* mutex, t_bool reacquire_mutex) = 0;
    virtual void signal(smp_mutex* mutex) = 0;
};

class smp_event
{
public:
    static smp_event* create(t_bool dothrow = TRUE);
    virtual ~smp_event() {}
    virtual void set() = 0;
    virtual void clear() = 0;
    virtual void wait() = 0;
    virtual t_bool trywait() = 0;
    virtual t_bool timed_wait(uint32 usec, uint32* p_actual_usec) = 0;
    virtual void wait_and_clear() = 0;
};

void smp_show_thread_priority_info(SMP_FILE* st);

/* =============================================== Internal definitions =============================================== */

#if defined(SIM_THREADS_H_FULL_INCLUDE)

#if defined(_WIN32)
#  define _WIN32_WINNT 0x0403
#  include <windows.h>
#  include <tchar.h>
#  include <process.h>
#elif (defined(__linux) || defined(__APPLE__)) && (defined(__x86_32__) || defined(__x86_64__))
#  include <stdio.h>
#  include <stdlib.h>
#  include <pthread.h>
#  include <unistd.h>
#  include <semaphore.h>
#  include <fcntl.h>
#  include <poll.h>
#  include <errno.h>
#endif

/*
 * OS X does not support unnamed semaphores. Functions sem_init and sem_destroy are declared, but always
 * return error code "Function not implemented". For this reason we have to use named semaphores under OS X
 * and provide a wrapper that insulates between the use of unnamed semaphores under Linux and
 * named semaphores under OS X.
 */
#if defined(__linux)
#  define os_sem_declare(semaphore)        \
          sem_t* semaphore;                \
          sem_t  semaphore##_holder
#  define os_sem_decl_init(semaphore)      \
          semaphore = NULL
#  define os_sem_ptr(semaphore)            \
          & (semaphore), &(semaphore##_holder)
#elif defined(__APPLE__)
#  define SEM_MAX_NAME (8 + 8 + 16 + 10)
#  define os_sem_declare(semaphore)        \
          sem_t* semaphore;                \
          char   semaphore##_name[SEM_MAX_NAME]
#  define os_sem_decl_init(semaphore)      \
          do { semaphore = NULL; semaphore##_name[0] = '\0'; } while (0)
#  define os_sem_ptr(semaphore)            \
          & (semaphore), semaphore##_name
#endif

#if (defined(__linux) || defined(__APPLE__)) && (defined(__x86_32__) || defined(__x86_64__))
#  define USE_SIMH_SMP_LOCK
#endif

#if defined(_WIN32)
#  define USE_SIMH_SMP_LOCK
#endif

class SIM_ALIGN_CACHELINE smp_lock_impl : public smp_lock
{
public:
    smp_lock_impl();
    ~smp_lock_impl();
    t_bool init(uint32 cycles = 0, t_bool dothrow = TRUE);
    t_bool init(uint32 us, uint32 min_cycles, uint32 max_cycles, t_bool dothrow = TRUE);
    void set_spin_count(uint32 cycles);
    void set_spin_count(uint32 us, uint32 min_cycles, uint32 max_cycles);
    void set_criticality(sim_lock_criticality_t criticality) { this->criticality = criticality; }
    void lock();
    void unlock();
    static void* operator new(size_t size)    { return operator_new_aligned(size, SMP_MAXCACHELINESIZE); }
    static void  operator delete(void* p)     { operator_delete_aligned(p); }
    static void calibrate();

public:
    static uint32 spins_per_ms;

protected:
    sim_lock_criticality_t criticality;

#if defined(_WIN32) && !defined(USE_SIMH_SMP_LOCK)
protected:
    SIM_ALIGN_32 CRITICAL_SECTION m_cs;
    t_bool m_inited;

#elif defined(USE_SIMH_SMP_LOCK)
public:
    void set_perf_collect(t_bool collect);
    void perf_reset();
    void perf_show(SMP_FILE* fp, const char* name);

protected:
    void perf_acquired(uint32 cycles);

protected:
    smp_interlocked_int32_var lock_count;
    volatile int32 recursion_count;
    volatile uint32 spin_count;
#if defined(_WIN32)
    volatile DWORD owning_thread;
    HANDLE semaphore;
#else
    volatile pthread_t owning_thread;
    os_sem_declare(semaphore);
#endif
    t_bool inited;

protected:
    void calibrate_spinloop();
    t_bool calibrating_spinloop;

protected:
    t_bool perf_collect;
    UINT64 perf_lock_count;
    UINT64 perf_nowait_count;
    UINT64 perf_syswait_count;
    double perf_avg_spinwait;
#endif

    /* dummy padding to prevent other data falling into the same cache line as lock data */
    t_byte pad[SMP_MAXCACHELINESIZE];
};

/* =============================================== Internal - Win32 =============================================== */

#if defined(_WIN32)
class smp_semaphore_impl : public smp_semaphore
{
private:
    volatile HANDLE hSemaphore;

public:
    smp_semaphore_impl();
    ~smp_semaphore_impl();
    t_bool init(int initial_open_count, t_bool dothrow = TRUE);
    void clear();
    void wait();
    t_bool trywait();
    void release(int count = 1);
    smp_pollable_handle_t pollable_handle();
    const char* pollable_handle_op();
};

#define SMP_BARRIER_CYCLES 4

class smp_barrier_impl : public smp_barrier
{
private:
    volatile HANDLE hMutex;
    volatile HANDLE hEvent;
    volatile int barrier_count;
    volatile int waiting_count;
    volatile HANDLE hOldEvents[SMP_BARRIER_CYCLES];
    void dealloc();
    void rotate_events();

public:
    smp_barrier_impl();
    virtual ~smp_barrier_impl();
    void wait();
    t_bool init(int initial_count, t_bool dothrow = TRUE);
    t_bool set_count(int count, t_bool dothrow = TRUE);
    void clear();
    t_bool trywait();
    void release(int count = 1);
};

class smp_mutex_impl: public smp_mutex
{
    friend class smp_condvar_impl;
private:
    HANDLE hMutex;
    sim_lock_criticality_t criticality;
public:
    smp_mutex_impl();
    ~smp_mutex_impl();
    t_bool init(t_bool dothrow);
    void set_criticality(sim_lock_criticality_t criticality);
    void lock();
    void unlock();
    t_bool trylock();
};

class smp_condvar_impl : public smp_condvar
{
private:
    HANDLE hEvent;
public:
    smp_condvar_impl();
    ~smp_condvar_impl();
    t_bool init(t_bool dothrow);
    void prepareForWait();
    void wait(smp_mutex* mutex, t_bool reacquire_mutex);
    void signal(smp_mutex* mutex);
};

class smp_event_impl : public smp_event
{
private:
    HANDLE hEvent;
    sim_delta_timer* delta_timer;
public:
    smp_event_impl();
    ~smp_event_impl();
    t_bool init(t_bool dothrow);
    void set();
    void clear();
    void wait();
    t_bool trywait();
    t_bool timed_wait(uint32 usec, uint32* p_actual_usec);
    void wait_and_clear();
};

// end of _WIN32
#endif

/* ============================================= Internal - Linux / OSX ============================================= */

#if defined(__linux) || defined(__APPLE__)

#define PIPE_RD 0
#define PIPE_WR 1

class smp_semaphore_impl : public smp_semaphore
{
private:
    int fd[2];

public:
    smp_semaphore_impl();
    ~smp_semaphore_impl();
    t_bool init(int initial_open_count, t_bool dothrow = TRUE);
    void clear();
    void wait();
    t_bool trywait();
    void release(int count = 1);
    smp_pollable_handle_t pollable_handle();
    const char* pollable_handle_op();
};

class smp_simple_semaphore_impl : public smp_simple_semaphore
{
private:
    t_bool inited;
    os_sem_declare(semaphore);

public:
    smp_simple_semaphore_impl();
    ~smp_simple_semaphore_impl();
    t_bool init(int initial_open_count, t_bool dothrow = TRUE);
    void clear();
    void wait();
    t_bool trywait();
    void release(int count = 1);
};

#define SMP_BARRIER_CYCLES 4

class smp_barrier_impl : public smp_barrier
{
private:
    pthread_mutex_t mutex;
    pthread_cond_t cond[SMP_BARRIER_CYCLES];

    t_bool mutex_inited;
    t_bool cond_inited[SMP_BARRIER_CYCLES];

    volatile int wait_index;
    volatile int barrier_count;
    volatile int waiting_count;
    volatile uint32 wseq;

    void dealloc();

public:
    smp_barrier_impl();
    virtual ~smp_barrier_impl();
    void wait();
    t_bool init(int initial_count, t_bool dothrow = TRUE);
    t_bool set_count(int count, t_bool dothrow = TRUE);
    void clear();
    t_bool trywait();
    void release(int count = 1);
};

class smp_mutex_impl: public smp_mutex
{
    friend class smp_condvar_impl;
private:
    pthread_mutex_t mutex;
    sim_lock_criticality_t criticality;
    t_bool inited;
public:
    smp_mutex_impl();
    ~smp_mutex_impl();
    t_bool init(t_bool dothrow);
    void set_criticality(sim_lock_criticality_t criticality);
    void lock();
    void unlock();
    t_bool trylock();
};

class smp_condvar_impl : public smp_condvar
{
private:
    pthread_cond_t cond;
    t_bool inited;
    volatile uint32 wseq;
public:
    smp_condvar_impl();
    ~smp_condvar_impl();
    t_bool init(t_bool dothrow);
    void prepareForWait();
    void wait(smp_mutex* mutex, t_bool reacquire_mutex);
    void signal(smp_mutex* mutex);
};

class smp_event_impl : public smp_event
{
private:
    pthread_mutex_t mutex;
    pthread_cond_t cond;
    t_bool inited;
#if defined(HAVE_POSIX_CLOCK_ID)
    t_bool have_clock_id;
    clockid_t clock_id;
#endif
    volatile t_bool state;
    volatile uint32 set_wseq;
public:
    smp_event_impl();
    ~smp_event_impl();
    t_bool init(t_bool dothrow);
    void set();
    void clear();
    void wait();
    t_bool trywait();
    t_bool timed_wait(uint32 usec, uint32* p_actual_usec);
    void wait_and_clear();
};

// end of __linux or __APPLE__
#endif

// end of SIM_THREADS_H_FULL_INCLUDE
#endif
